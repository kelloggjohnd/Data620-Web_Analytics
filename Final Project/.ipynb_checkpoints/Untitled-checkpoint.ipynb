{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "apart-pendant",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\x\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import nltk.corpus\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option(\"display.max_rows\",500)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "painful-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "activated-mining",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_csv('data/corpus.csv')\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vulnerable-lunch",
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches = pd.read_csv('data/presidential_speeches.csv')\n",
    "speeches.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuing-shame",
   "metadata": {},
   "source": [
    "mention or show example of Rscript to divide text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "furnished-situation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_features(document): \n",
    "    document_words = set(document) \n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "single-entrepreneur",
   "metadata": {},
   "outputs": [],
   "source": [
    "ob=open('data/obama1.txt','rU')\n",
    "raw_o=ob.read()\n",
    "raw_o=raw_o.lower()\n",
    "word_tokens = word_tokenize(raw_o)\n",
    "obama = [word for word in word_tokens if word not in stop_words]\n",
    "obama = [word for word in obama if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "coordinated-hartford",
   "metadata": {},
   "outputs": [],
   "source": [
    "wa=open('data/washington.txt','rU')\n",
    "raw_wa=wa.read()\n",
    "raw_wa=raw_wa.lower()\n",
    "word_tokens = word_tokenize(raw_wa)\n",
    "washington = [word for word in word_tokens if word not in stop_words]\n",
    "washington = [word for word in washington if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dirty-benjamin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98.413"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(obama)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "variable-projection",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.751"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(washington)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "natural-cemetery",
   "metadata": {},
   "outputs": [],
   "source": [
    "obama1=[]\n",
    "for i in range(98):\n",
    "    obama1.append([obama[i*1000:(i+1)*1000], 'ob'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "strong-aviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "washington1=[]\n",
    "for i in range(14):\n",
    "    washington1.append([washington[i*1000:(i+1)*1000], 'wa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "manufactured-endorsement",
   "metadata": {},
   "outputs": [],
   "source": [
    "wo = obama+washington"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "strategic-cattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches = washington1+obama1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "narrow-veteran",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = nltk.FreqDist(w.lower() for w in wo)\n",
    "word_features = list(all_words)[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "involved-click",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(speeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "solid-newton",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuresets = [(document_features(d), c) for (d,c) in speeches]\n",
    "len(featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "under-oklahoma",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = featuresets[50:], featuresets[:50]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "several-alcohol",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rubber-valentine",
   "metadata": {},
   "source": [
    "roosevelt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "palestinian-compatibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "ro=open('data/roosevelt.txt','rU')\n",
    "raw_ro=ro.read()\n",
    "raw_ro=raw_ro.lower()\n",
    "word_tokens = word_tokenize(raw_ro)\n",
    "roosevelt = [word for word in word_tokens if word not in stop_words]\n",
    "roosevelt = [word for word in roosevelt if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dietary-baltimore",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63.06"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(roosevelt)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "wicked-broadcast",
   "metadata": {},
   "outputs": [],
   "source": [
    "roosevelt1=[]\n",
    "for i in range(63):\n",
    "    roosevelt1.append([roosevelt[i*1000:(i+1)*1000], 'ro'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "senior-invalid",
   "metadata": {},
   "outputs": [],
   "source": [
    "rwo = obama+washington+roosevelt\n",
    "speeches = washington1+obama1+roosevelt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "numerical-silly",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = nltk.FreqDist(w.lower() for w in rwo)\n",
    "word_features = list(all_words)[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "northern-fraud",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(speeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "interesting-birth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuresets = [(document_features(d), c) for (d,c) in speeches]\n",
    "len(featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "coupled-relief",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.98\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = featuresets[50:], featuresets[:50]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ahead-austria",
   "metadata": {},
   "source": [
    "Kennedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "contemporary-comment",
   "metadata": {},
   "outputs": [],
   "source": [
    "ke=open('data/kennedy.txt','rU')\n",
    "raw_ke=ke.read()\n",
    "raw_ke=raw_ke.lower()\n",
    "word_tokens = word_tokenize(raw_ke)\n",
    "kennedy = [word for word in word_tokens if word not in stop_words]\n",
    "kennedy = [word for word in kennedy if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "representative-yield",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58.116"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kennedy)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "christian-generator",
   "metadata": {},
   "outputs": [],
   "source": [
    "kennedy1=[]\n",
    "for i in range(58):\n",
    "    kennedy1.append([kennedy[i*1000:(i+1)*1000], 'ke'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ongoing-remedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "rwok = obama+washington+roosevelt+kennedy\n",
    "speeches = washington1+obama1+roosevelt1+kennedy1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "behavioral-compromise",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = nltk.FreqDist(w.lower() for w in rwok)\n",
    "word_features = list(all_words)[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "superior-oklahoma",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "233"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.shuffle(speeches)\n",
    "featuresets = [(document_features(d), c) for (d,c) in speeches]\n",
    "len(featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "specific-grant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = featuresets[50:], featuresets[:50]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "permanent-notion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regular-renewal",
   "metadata": {},
   "source": [
    "Clinton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aboriginal-camcorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl=open('data/clinton.txt','rU')\n",
    "raw_cl=cl.read()\n",
    "raw_cl=raw_cl.lower()\n",
    "word_tokens = word_tokenize(raw_cl)\n",
    "clinton = [word for word in word_tokens if word not in stop_words]\n",
    "clinton = [word for word in clinton if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "interstate-filing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66.506"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clinton)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "stopped-token",
   "metadata": {},
   "outputs": [],
   "source": [
    "clinton1=[]\n",
    "for i in range(58):\n",
    "    clinton1.append([clinton[i*1000:(i+1)*1000], 'ke'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "transparent-heading",
   "metadata": {},
   "outputs": [],
   "source": [
    "rwokc = obama+washington+roosevelt+kennedy+clinton\n",
    "speeches = washington1+obama1+roosevelt1+kennedy1+clinton1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "developmental-interview",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = nltk.FreqDist(w.lower() for w in rwokc)\n",
    "word_features = list(all_words)[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "prerequisite-youth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "291"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.shuffle(speeches)\n",
    "featuresets = [(document_features(d), c) for (d,c) in speeches]\n",
    "len(featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "immune-copying",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = featuresets[50:], featuresets[:50]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "local-running",
   "metadata": {},
   "source": [
    "Reagan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "complex-stage",
   "metadata": {},
   "outputs": [],
   "source": [
    "re=open('data/reagan.txt','rU')\n",
    "raw_re=re.read()\n",
    "raw_re=raw_re.lower()\n",
    "word_tokens = word_tokenize(raw_re)\n",
    "reagan = [word for word in word_tokens if word not in stop_words]\n",
    "reagan = [word for word in reagan if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "south-technique",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.661"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reagan)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "biblical-blair",
   "metadata": {},
   "outputs": [],
   "source": [
    "reagan1=[]\n",
    "for i in range(58):\n",
    "    reagan1.append([reagan[i*1000:(i+1)*1000], 'ke'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "young-retention",
   "metadata": {},
   "outputs": [],
   "source": [
    "rwokcr = obama+washington+roosevelt+kennedy+clinton+reagan\n",
    "speeches = washington1+obama1+roosevelt1+kennedy1+clinton1+reagan1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "substantial-municipality",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = nltk.FreqDist(w.lower() for w in rwokcr)\n",
    "word_features = list(all_words)[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "assisted-center",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "349"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.shuffle(speeches)\n",
    "featuresets = [(document_features(d), c) for (d,c) in speeches]\n",
    "len(featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "corresponding-wagner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.84\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = featuresets[50:], featuresets[:50]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
