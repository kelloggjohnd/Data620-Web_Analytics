{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "handmade-pilot",
   "metadata": {},
   "source": [
    "# Project 3 - Text Mining\n",
    "\n",
    "Predict Name-Gender Labels with NLTK \"names\" data.\n",
    "\n",
    "---\n",
    "\n",
    "Jeff Shamp, John Kellogg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "greek-oxide",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/Users/jeffshamp/.conda/envs/sps620/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "[nltk_data] Downloading package names to /Users/jeffshamp/nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "\n",
    "#nltk.download('names')\n",
    "nltk.download('names')\n",
    "from nltk.corpus import names\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings;\n",
    "warnings.filterwarnings('ignore');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-track",
   "metadata": {},
   "source": [
    "## Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-figure",
   "metadata": {},
   "source": [
    "Using any of the three classifiers described in chapter 6 of Natural Language Processing with Python, and any features you can think of, build the best name gender classifier you can. \n",
    "\n",
    "Begin by splitting the Names Corpus into three subsets: \n",
    "* 500 words for the test set, \n",
    "* 500 words for the dev-test set \n",
    "* 6900 words remaining for the training set. \n",
    "\n",
    "Starting with the example name gender classifier, make incremental improvements. Use the dev-test set to check your progress.\n",
    "\n",
    "Once you are satisfied with your classifier, check its final performance on the test set. \n",
    "\n",
    "* How does the performance on the test set compare to the performance on the dev-test set? \n",
    "* Is this what you'd expect?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "favorite-profit",
   "metadata": {},
   "source": [
    "## Data Processing & Extract Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grateful-robert",
   "metadata": {},
   "source": [
    "As in everything the first step is to arrange the data into a usable format, like a dataframe. Once there we can ensure there is nothing we need to fix and extract some features like min, max, and average length of name.  Each of these will be useful later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "regular-blind",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data\n",
    "names_dict = {'name':names.words()}\n",
    "names_df = pd.DataFrame(data= names_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "charged-controversy",
   "metadata": {},
   "outputs": [],
   "source": [
    "males = names.words('male.txt')\n",
    "females = names.words('female.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "clear-compensation",
   "metadata": {},
   "outputs": [],
   "source": [
    "unisex = set(males).intersection(set(females))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "spread-basketball",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_names(row):\n",
    "    if row in names.words('female.txt'):\n",
    "        return \"female\"\n",
    "    if row in names.words('male.txt'):\n",
    "        return \"male\"\n",
    "    else: return -1\n",
    "    \n",
    "def get_last_letter(row):\n",
    "    return row[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "center-ivory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not the most efficient use of python or lists or encoders, but it works\n",
    "names_df['label'] = names_df.name.apply(lambda x: label_names(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "every-farming",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_df['last_letter'] = names_df.name.apply(lambda x: get_last_letter(x))\n",
    "names_df = names_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "accessory-repeat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min length name: 2\n",
      "Average length name: 6.03285498489426\n",
      "Median length name: 6.0\n",
      "Max length name: 15\n"
     ]
    }
   ],
   "source": [
    "print(f\"Min length name: {names_df.name.apply(len).min()}\",\n",
    "      f\"Average length name: {names_df.name.apply(len).mean()}\",\n",
    "      f\"Median length name: {names_df.name.apply(len).median()}\",\n",
    "      f\"Max length name: {names_df.name.apply(len).max()}\",\n",
    "      sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boxed-prague",
   "metadata": {},
   "source": [
    "Now, we have some useful stats and a way to clearly define Male/Female.  The Mean/Median length is around 6 characters.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animal-stations",
   "metadata": {},
   "source": [
    "## Base Line Model (of this report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "involved-indian",
   "metadata": {},
   "source": [
    "The book uses a Naive Bayes classifer for determining name-gender congruence using a simple last letter scheme. The model shown in the book achieved a 0.782 accuracy. We intend to beat that. \n",
    "\n",
    "We will leverage the diversity of name length to create character n-grams of the names and assign count scores to those character n-grams. Using the data from earlier, we see that the minimum length name is 2 characters and maximum is 15. \n",
    "\n",
    "Our process is to build n-grams to count from 0 - 15.  We found that this combination is likely to be the best for model accuracy and precision in our goal to beat the book. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "existing-rabbit",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_vectorizer = CountVectorizer(analyzer='char', ngram_range=(0, 16))\n",
    "X = char_vectorizer.fit_transform(names_df.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "configured-mitchell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse to sparse matrix\n",
    "X = X.tocsc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "equipped-kennedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dev = X[500:]\n",
    "test = X[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "clinical-lyric",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_dev, y_train, y_dev = \\\n",
    "train_test_split(train_dev,\n",
    "                  names_df['label'][500:],\n",
    "                  test_size=0.07,\n",
    "                  random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "uniform-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_model = MultinomialNB().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "coral-narrative",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      female       0.84      0.93      0.88       362\n",
      "        male       0.79      0.60      0.68       160\n",
      "\n",
      "    accuracy                           0.83       522\n",
      "   macro avg       0.81      0.76      0.78       522\n",
      "weighted avg       0.82      0.83      0.82       522\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_dev, naive_model.predict(X_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dried-premiere",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[336  26]\n",
      " [ 64  96]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_dev, naive_model.predict(X_dev)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confidential-source",
   "metadata": {},
   "source": [
    "### Initial Results\n",
    "\n",
    "Success! We have beat the book in all parameters.  The accuracy numbers are around 0.85\n",
    "\n",
    "Using a similar (or same family of classifiers) model we can achieve much higher accuracy on predicting name-gender using character n-grams than what was used in the book. Let's push it further.  Can we get those numbers up higher?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addressed-metro",
   "metadata": {},
   "source": [
    "## Potential other models\n",
    "\n",
    "### SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "right-family",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = SVC(gamma='scale').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "marine-assist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      female       0.84      0.95      0.89       362\n",
      "        male       0.84      0.57      0.68       160\n",
      "\n",
      "    accuracy                           0.84       522\n",
      "   macro avg       0.84      0.76      0.79       522\n",
      "weighted avg       0.84      0.84      0.83       522\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_dev, svm_model.predict(X_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "expensive-japanese",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[345  17]\n",
      " [ 68  92]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_dev, svm_model.predict(X_dev)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confidential-summit",
   "metadata": {},
   "source": [
    "#### SVM is no better than the base line model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-associate",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "widespread-running",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_model = LogisticRegression(max_iter=3000).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bigger-dubai",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      female       0.90      0.95      0.93       362\n",
      "        male       0.87      0.77      0.82       160\n",
      "\n",
      "    accuracy                           0.89       522\n",
      "   macro avg       0.89      0.86      0.87       522\n",
      "weighted avg       0.89      0.89      0.89       522\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_dev, logit_model.predict(X_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cooked-retail",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[344  18]\n",
      " [ 37 123]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_dev, logit_model.predict(X_dev)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel-attitude",
   "metadata": {},
   "source": [
    "#### So far the Logistic Regression model is the best in terms of precision and accuracy.  It also minimizes False Negatives/False Positive ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suitable-consultation",
   "metadata": {},
   "source": [
    "### GMB Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "impaired-houston",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmb_model = GradientBoostingClassifier().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "herbal-cookie",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      female       0.79      0.96      0.87       362\n",
      "        male       0.82      0.43      0.57       160\n",
      "\n",
      "    accuracy                           0.80       522\n",
      "   macro avg       0.81      0.69      0.72       522\n",
      "weighted avg       0.80      0.80      0.77       522\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_dev, gmb_model.predict(X_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "adapted-assault",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[347  15]\n",
      " [ 91  69]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_dev, gmb_model.predict(X_dev)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollywood-terrace",
   "metadata": {},
   "source": [
    "#### The GMB model is lower than the baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italian-luxury",
   "metadata": {},
   "source": [
    "## Development Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grand-question",
   "metadata": {},
   "source": [
    "We see that the best model is the Logistic Regression model as it is significantly better than both the book's and our baseline model. The more complex (and generally superior) Tree Boosted model was worse than the both base models. \n",
    "\n",
    "If constrained to a Naive Bayesian model only, our model trained on character n-grams demonstrated better results (86% accuracy) as compared to the book's base model (78%). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decreased-poetry",
   "metadata": {},
   "source": [
    "## Test Set Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flush-shore",
   "metadata": {},
   "source": [
    "We will now answer the initial question: \"How does the performance on the test set compare to the performance on the dev-test set?\"  Using the LogRegression model as our best dev model we fit it to the entire training set and evaluate on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "super-opinion",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = LogisticRegression(max_iter=3000).fit(train_dev, names_df.label[500:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "adjustable-savings",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      female       0.88      0.94      0.91       343\n",
      "        male       0.85      0.73      0.79       157\n",
      "\n",
      "    accuracy                           0.88       500\n",
      "   macro avg       0.87      0.84      0.85       500\n",
      "weighted avg       0.87      0.88      0.87       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(names_df.label[:500], final_model.predict(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "limited-today",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[323  20]\n",
      " [ 42 115]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(names_df.label[:500], final_model.predict(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seven-wrestling",
   "metadata": {},
   "source": [
    "The secondary question: \"Is this what you'd expect?\" is Yes, this is what was expected.  We achieved consistent results from the development set into the training/test set. While the numbers are lower than the dev numbers, they are still greater than the book's model numbers and still slightly higher than our base model numbers.\n",
    "\n",
    "Great!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-presentation",
   "metadata": {},
   "source": [
    "[Video Submission](!https://youtu.be/pQiAWf8XyEw)."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
